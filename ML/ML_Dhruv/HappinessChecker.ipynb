{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dhruv\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1583 - mae: 0.3560 - val_loss: 0.1551 - val_mae: 0.3531\n",
      "Epoch 2/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1584 - mae: 0.3586 - val_loss: 0.1553 - val_mae: 0.3572\n",
      "Epoch 3/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1605 - mae: 0.3620 - val_loss: 0.1569 - val_mae: 0.3600\n",
      "Epoch 4/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1587 - mae: 0.3589 - val_loss: 0.1549 - val_mae: 0.3532\n",
      "Epoch 5/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1576 - mae: 0.3570 - val_loss: 0.1551 - val_mae: 0.3568\n",
      "Epoch 6/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1595 - mae: 0.3608 - val_loss: 0.1548 - val_mae: 0.3560\n",
      "Epoch 7/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1598 - mae: 0.3606 - val_loss: 0.1550 - val_mae: 0.3567\n",
      "Epoch 8/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1599 - mae: 0.3610 - val_loss: 0.1565 - val_mae: 0.3594\n",
      "Epoch 9/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1578 - mae: 0.3590 - val_loss: 0.1546 - val_mae: 0.3550\n",
      "Epoch 10/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1593 - mae: 0.3597 - val_loss: 0.1548 - val_mae: 0.3558\n",
      "Epoch 11/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1584 - mae: 0.3592 - val_loss: 0.1548 - val_mae: 0.3558\n",
      "Epoch 12/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1568 - mae: 0.3569 - val_loss: 0.1550 - val_mae: 0.3569\n",
      "Epoch 13/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1551 - mae: 0.3550 - val_loss: 0.1548 - val_mae: 0.3553\n",
      "Epoch 14/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1576 - mae: 0.3579 - val_loss: 0.1548 - val_mae: 0.3549\n",
      "Epoch 15/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1554 - mae: 0.3551 - val_loss: 0.1557 - val_mae: 0.3581\n",
      "Epoch 16/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1574 - mae: 0.3581 - val_loss: 0.1550 - val_mae: 0.3564\n",
      "Epoch 17/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1573 - mae: 0.3581 - val_loss: 0.1549 - val_mae: 0.3543\n",
      "Epoch 18/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1573 - mae: 0.3572 - val_loss: 0.1549 - val_mae: 0.3552\n",
      "Epoch 19/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1570 - mae: 0.3574 - val_loss: 0.1552 - val_mae: 0.3568\n",
      "Epoch 20/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1592 - mae: 0.3603 - val_loss: 0.1549 - val_mae: 0.3559\n",
      "Epoch 21/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1568 - mae: 0.3569 - val_loss: 0.1554 - val_mae: 0.3572\n",
      "Epoch 22/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1568 - mae: 0.3571 - val_loss: 0.1549 - val_mae: 0.3553\n",
      "Epoch 23/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1547 - mae: 0.3545 - val_loss: 0.1550 - val_mae: 0.3557\n",
      "Epoch 24/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1568 - mae: 0.3568 - val_loss: 0.1552 - val_mae: 0.3559\n",
      "Epoch 25/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1549 - mae: 0.3544 - val_loss: 0.1551 - val_mae: 0.3565\n",
      "Epoch 26/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1574 - mae: 0.3578 - val_loss: 0.1550 - val_mae: 0.3560\n",
      "Epoch 27/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1561 - mae: 0.3565 - val_loss: 0.1558 - val_mae: 0.3581\n",
      "Epoch 28/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1581 - mae: 0.3594 - val_loss: 0.1548 - val_mae: 0.3545\n",
      "Epoch 29/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1580 - mae: 0.3585 - val_loss: 0.1554 - val_mae: 0.3571\n",
      "Epoch 30/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1568 - mae: 0.3572 - val_loss: 0.1549 - val_mae: 0.3548\n",
      "Epoch 31/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1579 - mae: 0.3583 - val_loss: 0.1555 - val_mae: 0.3570\n",
      "Epoch 32/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1599 - mae: 0.3612 - val_loss: 0.1551 - val_mae: 0.3551\n",
      "Epoch 33/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1602 - mae: 0.3610 - val_loss: 0.1549 - val_mae: 0.3557\n",
      "Epoch 34/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1597 - mae: 0.3604 - val_loss: 0.1550 - val_mae: 0.3561\n",
      "Epoch 35/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1589 - mae: 0.3594 - val_loss: 0.1552 - val_mae: 0.3550\n",
      "Epoch 36/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1561 - mae: 0.3559 - val_loss: 0.1556 - val_mae: 0.3566\n",
      "Epoch 37/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1560 - mae: 0.3558 - val_loss: 0.1551 - val_mae: 0.3564\n",
      "Epoch 38/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1566 - mae: 0.3572 - val_loss: 0.1558 - val_mae: 0.3576\n",
      "Epoch 39/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1556 - mae: 0.3559 - val_loss: 0.1553 - val_mae: 0.3567\n",
      "Epoch 40/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1581 - mae: 0.3587 - val_loss: 0.1550 - val_mae: 0.3559\n",
      "Epoch 41/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1578 - mae: 0.3586 - val_loss: 0.1550 - val_mae: 0.3555\n",
      "Epoch 42/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1561 - mae: 0.3561 - val_loss: 0.1551 - val_mae: 0.3534\n",
      "Epoch 43/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1602 - mae: 0.3605 - val_loss: 0.1553 - val_mae: 0.3564\n",
      "Epoch 44/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1598 - mae: 0.3608 - val_loss: 0.1549 - val_mae: 0.3553\n",
      "Epoch 45/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1578 - mae: 0.3583 - val_loss: 0.1552 - val_mae: 0.3560\n",
      "Epoch 46/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1579 - mae: 0.3578 - val_loss: 0.1550 - val_mae: 0.3560\n",
      "Epoch 47/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1567 - mae: 0.3570 - val_loss: 0.1548 - val_mae: 0.3545\n",
      "Epoch 48/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1560 - mae: 0.3551 - val_loss: 0.1553 - val_mae: 0.3557\n",
      "Epoch 49/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1564 - mae: 0.3562 - val_loss: 0.1554 - val_mae: 0.3571\n",
      "Epoch 50/50\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1550 - mae: 0.3555 - val_loss: 0.1555 - val_mae: 0.3564\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n",
      "Predicted Happiness Score: [[0.30919254]]\n",
      "Day of the Week: Thursday\n",
      "Friend Interacted With: Family_6\n",
      "Preferred Time Slot: Morning\n",
      "Predicted Happiness Score: 0.30919254\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Model Performance - MAE: 0.3564, MSE: 0.1555\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"personalized_social_scheduler_10k.csv\")\n",
    "\n",
    "# Compute Happiness Score (Ensure it remains above 60%)\n",
    "df[\"Happiness_Score\"] = (df[\"Mood_After\"] - df[\"Mood_Before\"]) * 10 + 60\n",
    "df[\"Happiness_Score\"] = df[\"Happiness_Score\"].clip(lower=60, upper=100)\n",
    "\n",
    "# Compute Social Interaction Score (Normalize & Prevent Excess)\n",
    "df[\"Social_Interaction_Score\"] = (df[\"Interaction_Frequency_Score\"] + df[\"Social_Preference_Score\"]) / 2\n",
    "df[\"Social_Interaction_Score\"] = df[\"Social_Interaction_Score\"].clip(upper=8)\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for col in ['Day_of_Week', 'Friend_ID', 'Preferred_Time_Slot']:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Scale numerical values\n",
    "scaler = MinMaxScaler()\n",
    "numerical_cols = ['Happiness_Score', 'Social_Interaction_Score']\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Define features & target\n",
    "X = df[['Day_of_Week', 'Friend_ID', 'Preferred_Time_Slot', 'Social_Interaction_Score']]\n",
    "y = df['Happiness_Score']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape for LSTM input\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(1, X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Normalize output\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Test with a sample input\n",
    "sample_input = np.array([[4, 12, 2, 5]])  # Random values for testing\n",
    "sample_input = np.expand_dims(sample_input, axis=1)\n",
    "predicted_happiness = model.predict(sample_input)\n",
    "\n",
    "\n",
    "# Decode categorical values back to original labels\n",
    "decoded_day = label_encoders['Day_of_Week'].inverse_transform([4])[0]\n",
    "decoded_friend = label_encoders['Friend_ID'].inverse_transform([12])[0]\n",
    "decoded_time_slot = label_encoders['Preferred_Time_Slot'].inverse_transform([2])[0]\n",
    "\n",
    "# Print meaningful results\n",
    "print(f\"Predicted Happiness Score: {predicted_happiness}\")\n",
    "print(f\"Day of the Week: {decoded_day}\")\n",
    "print(f\"Friend Interacted With: {decoded_friend}\")\n",
    "print(f\"Preferred Time Slot: {decoded_time_slot}\")\n",
    "# Print results\n",
    "print(\"Predicted Happiness Score:\", predicted_happiness[0][0])\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Model Performance - MAE: {mae:.4f}, MSE: {mse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
